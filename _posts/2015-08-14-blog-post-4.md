---
title: 'Re-Thinking Inverse Graphics With Large Language Models'
date: 04/2024
permalink: /posts/2012/08/ig-llm-egemenkaya/
tags:
  - Inverse Graphics
  - LLMs
  - Spatial Reasoning
---

Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate scenes that are only available as single 2D/3D images. 
The biggest problem with IG is that it is very difficult to generalize features such as comprehensive understanding of a 3D environment's spatial and physical properties. This limits their ability to generalize hugely. In the recent past, different approaches to solve this problem has ben researched. This research's goal is to find a new approach to solving this problem, thus creating graphics programs that are able to reproduce the scenes using a traditional graphics engine.

The authors are planning to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. In order to achieve this, they propose the Inverse-Graphics Large Language Model (IG-LLM): an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene.

<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>

Why Large Language Models?
---

Too understand what IG-LLM does differently than other approaches that try to develop inverse graphics methods, we must first know what progress was made before IG-LLM. Though there are multiple approaches, they all have some features in common, such as analysing every object in the scene indepentently, completely overlooking the semantics and interrelation between objects, hindering any reasoning. And the most common problem they all have is that they all have generalization problems because of the way they were constructed. That means they have to rely on task-spesific inductive biases or detailed and spesific training sets.

This is where LLMs come into importance. One of the greatest successes of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, the IG-LLMs could theoretically solve the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 
