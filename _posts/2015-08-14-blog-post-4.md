---
title: 'Re-Thinking Inverse Graphics With Large Language Models'
date: 04/2024
permalink: /posts/2012/08/ig-llm-egemenkaya/
tags:
  - Inverse Graphics
  - LLMs
  - Spatial Reasoning
---

Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate 3D scenes/environments from 2D images. It is a very common process, used in numerous fields, such as; Autonomous driving, computer vision, augmented reality and any system, where it is required to understand and process a 3D environment's properties through 2D images. 

<div align="center">
<img src="/images/teapot.png" alt="teapot" align="center" width="600"/>
</div>
*Figure sourced from the paper [Overcoming Occlusion with Inverse Graphic](https://link.springer.com/chapter/10.1007/978-3-319-49409-8_16#citeas).*

In the recent past, different approaches to solve this problem has been researched. In this blog post, we are going to take a look at a new approach that tries to solve more generalized inverse graphics problems: Inverse-Graphics Large Language Model (IG-LLM), from the paper "Re-Thinking Inverse Graphics With Large Language Models" by Peter Kulits, Haiwen Feng et al at ICCV 2023.

*Unless otherwise noted, all images in this post are from the paper [Re-Thinking Inverse Graphics With Large Language Models](https://arxiv.org/abs/2404.15228).*

<h3 id="why llms">What are the steps to "recreate" a scene?</h3>

<h3.5 id="generalization">Generalization and Classification</h3.5>

In order to recreate a scene/environment seen in a 2D picture, there are some core steps a model has to take.
  First, the picture needs to be classified and "understood". What that means is that the model has to be able to define the objects in the picture and tell them apart by a sort of generalization algorithm. There are many ways to accomplish this task such as; pixel-level, object-level and scene-level classification. Usually, features are extracted from the image using convolutional neural networks (CNNs) or other techniques. For now, this is all we need to know about how classification works.

<div align="center">
<img src="/images/classification.png" alt="classification" align="center" width="600"/>
</div>
*Figure sourced from the paper [Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities](https://www.researchgate.net/publication/342541335_Remote_Sensing_Image_Scene_Classification_Meets_Deep_Learning_Challenges_Methods_Benchmarks_and_Opportunities).*

<h3.5 id="holisticscene">Holistic Scene Understanding</h3.5>

  After understanding what the objectsin the image are, the relations between those objects must be inspected. Because the images are 2D planes with no depth, it is crucial to develop the understanding of depth before analyzing the spacing in the images. That is called 'holistic scene understanding' and how it operates is basically connecting objects with each others, using their spatial properties and other relations built before. This allows the models to understand the correct 3D setting of the objects from a 2D image.

<div align="center">
<img src="/images/holisticscene.png" alt="holisticscene" align="center" width="600"/>
</div>
*Figure sourced from the paper [Generating Visual Spatial Description via Holistic 3D Scene Understanding](https://arxiv.org/abs/2305.11768).*


<h3 id="why llms">Neural Scene De-Rendering</h3>

The most basic way to solve IG problems is by using [Neural Scene De-Rendering](https://jiajunwu.com/papers/nsd_cvpr.pdf). It utilizes a neural network, often a convolutional neural network (CNN) or a more advanced architecture like a [Generative Adversarial Network (GAN)](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) to analyze the image. First, a CNN or GAN is utilized to recignize features in the image such as edges of objects, colors and textures. Then, in the feature embedding layer, it converts those high-level features into a fixed-dimensional vector or embedding. After that, through the usage of recurrent neural networks (RNNs) or transformers, it translates the visual features into symbolic representations. And lastly, through multi-layer perceptrons (MLPs) and other specialized networks, it reconstructs the 3D scene from the symbolic and numeric information.

<div align="center">
<img src="/images/neuralscene.png" alt="neuralscene" align="center" width="600"/>
</div>
*Figure sourced from the paper [Neural Scene De-Rendering](https://jiajunwu.com/papers/nsd_cvpr.pdf).*


At this point it looks excellent. But if even the core approach of neural scene de-rendering is so capable by itself, why bother looking for other solutions to IG problems?
The answer to that question would be a single word: "generalizability". Even though the standard neural scene de-rendering approach by itself is already perfectly suited for many use-cases, it all falls apart when it is used with any kind of data it was not spesifically trained with. Because it is a simple framework/pipeline, it is not generalizable and does not have any zero-shot capabilities. 

The goal of the IG-LLM model is to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. It is an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene, and with that, has the potential to be more generalizable and accurate than all of the previous approaches.

<h3 id="why llms">Large Language Models in Vision-Language</h3>

As known, one of the greatest abilities of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data. There are many tasks in the domain of vision-language where LLMs are utilized such as: Generating textual descriptions from images or invice versa, visual question answering (VQA), visual instruction following etc.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, IG-LLM solves the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 

Inverse-Graphics Large Language Model
------

While LLMs create complete language-token sequences, past visual question answering (VQA) works have shown that large pretrained vision transformers can also be used as visual tokenizers. By fusing linguistic tokens for the LLM with visual embeddings, these studies integrate the understanding of images and words. They take an approach that is consistent with this method, building an LLM that can "see" the input image and provide a structured code representation of the input scene. IG-LLM is a model that takes advantage of this capability of LLMs and uses it to make logical reasonings that help through the process of understanding and recreating an image. 

<h3 id="architecture">Architecture of IG-LLM</h3>

The IG-LLM model is built upon the concept of neural scene de-rendering, with an instruction-tuned variant of LLaMA-1 7B in conjunction with a frozen CLIP vision encoder, used as the visual tokenizer. The layers of IG-LLM are as following:

* Region-proposal network for object detection
* Segmentation network for isolating the objects from the background
* Attribute network for classifying various discrete graphics attributes
* Localization network for predicting the object's spatial location
* Continuous numeric head
* An instruction tuned variant of LLaMA-1 7B

<div align="center">
<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>
</div>

<h3 id="architecture">How IG-LLM works</h3>

First, a pre-trained visual encoder, CLIP, generates visual embeddings from the 2D input image, representing object attributes like shape, coordinates, size, and color. Next, an instruction-tuned LLaMa-1 7B model translates these embeddings into graphics code. This process uses autoregressive prediction and contextual clues to translate object attributes in a logical sequence. The model handles textual properties (size, color, shape) and, simultaneously, a continuous numeric head, implemented as a four-layer Multi-Layer Perceptron (MLP), predicts non-discrete properties like location and rotation. A special [NUM] token in the vocabulary helps the LLM distinguish between generating numerical values and textual information. During training, Mean Squared Error (MSE) loss is used for numerical values, while next-token prediction loss is applied to the [NUM] token, as described in the equation below.

<div align="center">
<img src="/images/pifalanfilan.png" alt="formula1" align="center" width="230"/>
</div>

And lastly, the result image is rendered using the Blender modeling software from its Python scripting API, through the graphics code generated by IG-LLM.

<h3 id="trainingdata">Training Data Generation 'CLEVR'</h3>

The procedurally created dataset CLEVR consists of basic three-dimensional objects arranged on a plane. The shape (such as spheres, cubes, etc.), size, color, and spatial pose of these objects, also known as primitives, are all randomly chosen. Shape, size, color, and material are discrete qualities (for the LLM); on the other hand, the pose, which indicates the object's location and orientation, is a continuous property (for the numeric head). Unlike the original CLEVR data, the training data for IG-LLM is created with fixed camera positions, but the randomness in the lighting is still maintained.

<div align="center">
<img src="/images/cleversample.png" alt="clevrsample" align="center" width="600"/>
</div>

To train the model, pairs of codes and their corresponding images are given to the model, and the model is tasked with figuring what code could be used to reproduce the image in the given pair. After that, the model is continued to train with a traditional next-token prediction task, with the goal of maximizing the conditional probability of the next token.

<h3 id="training">Training IG-LLM and Vision-Language Alignment</h3>

The linear vision-encoder projection is initially trained using LLaVA's feature-alignment method with instruction sequences from image-caption pairs in the Conceptual Captions dataset. During this phase, the language model receives an image embedding from the vision encoder and a prompt to describe the image and caption, with only the vision projector being updated.

<div align="center">
<img src="/images/prompt.png" alt="prompt" align="center" width="600"/>
</div>

For training IG-LLM, several loss functions are used: perceptual loss for high-level image similarity, reconstruction loss for fidelity to the original images, and discriminator loss in a GAN framework to improve realism by distinguishing between real and synthetic images. The CLEVR and parameter-space generalization models are trained for 40k steps, while the 6-DoF pose-estimation models are trained for 200k steps.

Evaluations
------
The authors built many targeted evaluation settings to assess IG-LLM's generalization capacity across distribution alterations. To analyze model capability quantitatively under controlled shifts, they ran experiments using synthetic data.

<h3 id="compgenerclevr">Compositional Generarlization on CLEVR</h3>
The CLEVR-CoGenT benchmark is adapted to evaluate the compositional generalization capabilities of VQA models in an inverse-graphics problem domain. The adapted dataset tests models on in-distribution (ID) data, which adheres to familiar combinations of attributes, and out-of-distribution (OOD) data, which features novel attribute combinations not seen during training. 

In these tests, IG-LLM is compared against the base VQA model. During training, both models are trained with datasets that spesifically uses fixed combinations of attributes (red cubes, blue cylinders etc). In the OOD condition, the color palettes of the shapes are swapped except spheres, which always have all eight colors available. Both models are trained on 4k images from the ID condition and then evaluated on 1k images from both conditions.

<div align="center">
<img src="/images/vqaigllm.png " alt="vqaigllm" align="center" width="600"/>
</div>

They use linear-sum assignment on pairwise Euclidean distances to match predicted and ground-truth objects in order to assess attribute-recognition accuracy. To handle problems with missing or duplicated items, they further evaluate counting accuracy by computing the mean-absolute counting error between anticipated and ground-truth object sets.

<div align="center">
<img src="/images/comp.png" alt="composition" align="center" width="600"/>
</div>

Both IG-LLM and NS-VQA achieve over 99% accuracy on the in-distribution (ID) condition. However, on the out-of-distribution (OOD) condition, the NS-VQA's shape-recognition accuracy drops by 66.12%, while IG-LLM's accuracy decreases by only 6.01%. Without spheres, the VQA's accuracy falls to 0.03%.

<div align="center">
<img src="/images/floatchar.png" alt="floatcharcomp" align="center" width="600"/>
</div>

In terms of data efficiency, the float-based model is initially more efficient than the char-based model in estimating object positions, but this difference reduces with 4,000 samples. The char-based model is 6.41 times more likely to predict exact values seen during training.

<h3 id="numericparameterspace">Numeric Parameter-Space Generalization</h3>
2D Parameter Space

The test involved training the model on dots placed in a sparse checkerboard grid and evaluating it on dots uniformly sampled across the area. The float-based model significantly outperformed the char-based model, which overfit to the training data and struggled with out-of-distribution inputs. The float-based model adapted well to new distributions, showing a smaller performance gap and more stable validation mean squared error (MSE), despite a slight positional bias.

<div align="center">
<img src="/images/2D.png" alt="2d" align="center" width="600"/>
</div>
<br>

SO(3) Parameter Space

In evaluating SO(3)-pose estimation for orientable objects using toy-airplane assets from Super-CLEVR, the float-based model outperformed the char-based model in both in-distribution and out-of-distribution (OOD) scenarios. Trained on images with fixed-plane locations and random Euler rotations, the char-based model showed 2.64 times higher error than the float-based model in-distribution and a similar 2.52 times higher error OOD. The float-based modelâ€™s superior performance is attributed to its better handling of data dimensionality and efficiency in parameter-space generalization.

<div align="center">
<img src="/images/3D.png" alt="3d" align="center" width="400"/>
</div>

<h3 id="6dofpose">6-DoF Pose Estimation</h3>
In this section, two test cases for measuring the picking and posing capabilities of the model are conducted: Single-Object 6-DoF and Scene-Level 6-DoF. The dataset includes more-complex multi-object scenes and the models handle larger collections of 3D assets (>100).

In the single object tests, the number of colors used in the images is raised to 133 so that the emulations are closer to the real world. The object sizes are fixed and the models are trained using datasets with one million images. Using intrinsic-Euler for the char-based model and 6D representation for the float-based model, it is shown that both models achieve significantly lower positional and rotational errors compared to the CLEVR setting.

<div align="center">
<img src="/images/singleobjectpic.png" alt="singlepic" align="center" width="600"/>
</div>

<div align="center">
<img src="/images/singleobject.png" alt="singleobject" align="center" width="600"/>
</div>

In the scene level tests, the authors use an enlarged CLEVR-like dataset that includes ShapeNet objects to assess the scalability of IG-LLM for scene-level 6-DoF pose estimation with three to five objects per scene. This dataset uses an enlarged color set and varies camera pitch and radius. It contains 56 different types of chairs, 35 varieties of sofas, and 47 types of tables. Ten thousand training images are generated and evaluated under three scenarios: out-of-distribution texture (OOD-T), out-of-distribution texture and shape (OOD-T+S), and in-distribution (ID).

<div align="center">
<img src="/images/scenelevelpic.png" alt="scenelevelpic" align="center" width="600"/>
</div>

<div align="center">
<img src="/images/scenelevel.png" alt="scenelevel" align="center" width="600"/>
</div>

The float-based model often outperforms the char-based model, notably in the OOD-T+S condition. Performance decreases in the OOD-T condition, with notable increases in count error and decreases in shape-recognition accuracy. The model's tendency to represent multi-color textured objects as mixtures of solid-color assets is the cause of the performance decrease. Scene-level chamfer distance exhibits less noticeable degradation in spite of the drop.


Conclusion of IG-LLM
------
The expressiveness of IG-LLM is limited by the training-data-generation framework. While the model can learn to disentangle and reconstruct scenes under distribution shifts, it is constrained by the objects and language it was trained with. If it encounters unknown objects or names outside its training data, it cannot accurately use or render them. Additionally, in real world cases, the model can not work with most of the given images due to the lack of diverse camera angles in the training data, which is caused by the limitations of CLEVR data-generator.

However, IG-LLM taking advantage of LLMs means that any improvement seen in the LLM domain can effectively enhance its accuracy and capacity of understanding more complex real-world scenes with numerous objects and significant occlusions. With advancements made in the training-data generators, the model can also explore richer scene representations and refine its numeric estimation.

References
------
- Moreno, Pol & Williams, Christopher & Nash, Charlie & Kohli, Pushmeet. (2016). Overcoming Occlusion with Inverse Graphics. 10.1007/978-3-319-49409-8_16.
- Kulits, Peter & Feng, Haiwen & Liu, Weiyang & Abrevaya, Victoria & Black, Michael J. (2023). Re-Thinking Inverse Graphics With Large Language Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
- Zhao, Yu, Fei, Hao, Ji, Wei, Wei, Jianguo, Zhang, Meishan, Zhang, Min, Chua, Tat-Seng. (2023). Generating Visual Spatial Description via Holistic 3D Scene Understanding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7960-7977, Association for Computational Linguistics. DOI: 10.18653/v1/2023.acl-long.897.
- Cheng, Gong & Xie, Xingxing & Han, Junwei & Li, Kaiming & Xia, Gui-Song. (2020). Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 13. 3735-3756. 10.1109/JSTARS.2020.3005403.
- Wu, Jiajun, Tenenbaum, Joshua B., Kohli, Pushmeet. (2017). Neural Scene De-Rendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 699-707. DOI: 10.1109/CVPR.2017.540.
- Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In Proceedings of the 28th Conference on Neural Information Processing Systems (NeurIPS 2014) (pp. 2672-2680).
