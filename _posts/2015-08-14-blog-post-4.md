---
title: 'Re-Thinking Inverse Graphics With Large Language Models'
date: 04/2024
permalink: /posts/2012/08/ig-llm-egemenkaya/
tags:
  - Inverse Graphics
  - LLMs
  - Spatial Reasoning
---

Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate 3D scenes/environments from 2D images. It is a very common process, used in numerous fields, such as; Autonomous driving, computer vision, augmented reality and any system, where it is required to understand and process a 3D environment's properties through 2D images. 
In the recent past, different approaches to solve this problem has ben researched. In this blog post, we are going to take a look at three spesific approaches that try to to solve inverse graphics problems: Neural Scene De-Rendering, Visual Question Answering (VQA) and Inverse Graphics Large Language Model (IG-LLM), which is the research from the paper "Re-Thinking Inverse Graphics With Large Language Models".
Although many researches still are being conducted on the field of inverse graphics, it is safe to say that one of the core processes of inverse graphics comes from way before.

<img src="/images/teapot.png" alt="teapot" align="center" width="600"/>

<h3 id="why llms">What is needed to "recreate" a scene?</h3>

In order to recreate a scene/environment seen in a 2D picture, there are some core steps a model has to take.
  First, the picture needs to be classified and "understood". What that means is that the model has to be able to define the objects in the picture and tell them apart by a sort of generalization algorithm. There are many ways to accomplish this task such as; pixel-level, object-level and scene-level classification. Usually, features are extracted from the image using convolutional neural networks (CNNs) or other techniques. For now, this is all we need to know about how classification works.

<img src="/images/classification.png" alt="classification" align="center" width="600"/>

  After understanding what the objectsin the image are, the relations between those objects must be inspected. Because the images are 2D planes with no depth, it is crucial to develop the understanding of depth before analyzing the spacing in the images. That is called 'holistic scene de-rendering' and how it operates is basically connecting objects with each others, using their spatial properties and other relations built before. This allows the models to understand the correct 3D setting of the objects from a 2D image.


Although, the biggest problem with IG is that it is very difficult to generalize features such as comprehensive understanding of a 3D environment's spatial and physical properties. This limits their ability to generalize hugely. 

This research's goal is to find a new approach to solving this problem, thus creating graphics programs that are able to reproduce the scenes using a traditional graphics engine.

The authors are planning to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. In order to achieve this, they propose the Inverse-Graphics Large Language Model (IG-LLM): an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene.

<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>

<h3 id="why llms">Why Large Language Models?</h3>

To understand what IG-LLM does differently than other approaches that try to develop inverse graphics methods, we must first know what progress was made before IG-LLM. Though there are multiple approaches, they all have some features in common, such as analysing every object in the scene indepentently, completely overlooking the semantics and interrelation between objects, hindering any reasoning. And the most common problem they all have is that they all have generalization problems because of the way they were constructed. That means they have to rely on task-spesific inductive biases or detailed and spesific training sets.

This is where LLMs come into play. One of the greatest successes of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, IG-LLM solves the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 

Tuning LLMs for Inverse Graphics
------

While LLMs create complete language-token sequences, past visual question answering (VQA) works have shown that large pretrained vision transformers can also be used as visual tokenizers. By fusing linguistic tokens for the LLM with visual embeddings, these studies integrate the understanding of images and words. They take an approach that is consistent with this method, building an LLM that can "see" the input image and provide a structured code representation of the input scene.

<h3 id="architecture">Architecture of the IG-LLM</h3>

The IG-LLM model is based on an instruction-tuned variant of LLaMA-1 7B in conjunction with a frozen CLIP vision encoder, used as the visual tokenizer. 

<h3 id="training">Training the IG-LLM / Vision-Language Alignment</h3>

The linear vision-encoder projection is initially trained using the feature-alignment pre-training method from LLaVA. Instruction sequences created from image-caption pairings taken from the Conceptual Captions dataset are used in this training. During training, the language model (LLM) receives an image embedding produced by the vision encoder, along with a randomly selected prompt that instructs the model to describe both the image and its caption. Throughout this pre-training stage, all parameters of the model remain unchanged, except for the learnable vision projector.


<h3 id="trainingdata">Training Data Generation</h3>

The procedurally created dataset CLEVR consists of basic three-dimensional objects arranged on a plane. The shape (such as spheres, cubes, etc.), size, color, and spatial pose of these objects, also known as primitives, are all randomly chosen. Shape, size, color, and material are discrete qualities; on the other hand, the pose, which indicates the object's location and orientation, is a continuous property. The images from the sampled scenes are rendered using the Blender modeling software from its Python scripting API.

<img src="/images/cleversample.png" alt="clevrsample" align="center" width="600"/>

To train the model, pairs of codes and their corresponding images are given to the model, and the model is tasked with figuring what code could be used to reproduce the image in the given pair. After that, the model is continued to train with a traditional next-token prediction task, with the goal of maximizing the conditional probability of the next token.

<img src="/images/pifalanfilan.png" alt="formula1" align="center" width="200"/>

Where s_i represents the ith token.

<img src="/images/numerichead.png" alt="numerichead" align="center" width="600"/>

As you can observe in the image above, the "Numeric Head" concept is introduced to handle numbers differently in this model. Instead of having the model generate numbers as individual digits, it has been teached to produce a special [NUM] token whenever a number is needed. This [NUM] token acts as a placeholder, indicating that the number should be processed separately.

<h3 id="differences">Differences from Traditional Approaches</h3>

The framework introduced in this model differs from the traditional approaches in a big way. The visual-encoding process of this new framework does not work with graphics-spesific inductive biases. It does not go through any training for intermediate vision tasks, such as recognizing objects, segmentation or attribute regression. The model does not have access to the 3D models and thus only works with the rendered images. Although this approach might look less accurate at first, with further testing it is seen that this approach increases the shape-recognition accuracy by approximately 60% compared to the conventional ways.

<h3 id="numericreasoning">Precise Numeric Reasoning in LLMs</h3>

//TODO

Evaluations
------

//SIDENOTE: All of the text in this draft will be reconsidered and hopefully will be written better. This is just a draft.

//TODO: From this part on, I am planning to show the tests and their results in comparison with more conventional approaches, I will use the images at the end of the paper to talk about the differences in the results, and then write about my own opinions on this whole project. My teammate Safet and I will try to connect our blog posts to each other and write about the general problems, future directions, etc. of projects like LISA and IG-LLM.

<h3 id="compgenerclevr">Compositional Generarlization on CLEVR</h3>

<h3 id="numericparameterspace">Numeric Parameter-Space Generalization</h3>

<h3 id="6dofpose">6-DoF Pose Estimation</h3>

Conclusion
------
