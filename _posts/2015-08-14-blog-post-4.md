---
title: 'Re-Thinking Inverse Graphics With Large Language Models'
date: 04/2024
permalink: /posts/2012/08/ig-llm-egemenkaya/
tags:
  - Inverse Graphics
  - LLMs
  - Spatial Reasoning
---

Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate scenes that are only available as single 2D/3D images. 
The biggest problem with IG is that it is very difficult to generalize features such as comprehensive understanding of a 3D environment's spatial and physical properties. This limits their ability to generalize hugely. In the recent past, different approaches to solve this problem has ben researched. This research's goal is to find a new approach to solving this problem, thus creating graphics programs that are able to reproduce the scenes using a traditional graphics engine.

The authors are planning to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. In order to achieve this, they propose the Inverse-Graphics Large Language Model (IG-LLM): an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene.

<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>

<h3 id="why llms">Why Large Language Models?</h3>

To understand what IG-LLM does differently than other approaches that try to develop inverse graphics methods, we must first know what progress was made before IG-LLM. Though there are multiple approaches, they all have some features in common, such as analysing every object in the scene indepentently, completely overlooking the semantics and interrelation between objects, hindering any reasoning. And the most common problem they all have is that they all have generalization problems because of the way they were constructed. That means they have to rely on task-spesific inductive biases or detailed and spesific training sets.

This is where LLMs come into play. One of the greatest successes of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, IG-LLM solves the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 

<h4 id="how to integrate it">Tuning LLMs for Inverse Graphics</h4>

While LLMs create complete language-token sequences, past visual question answering (VQA) works have shown that large pretrained vision transformers can also be used as visual tokenizers. By fusing linguistic tokens for the LLM with visual embeddings, these studies integrate the understanding of images and words. They take an approach that is consistent with this method, building an LLM that can "see" the input image and provide a structured code representation of the input scene.

<h4 id="architecture">Architecture of the IG-LLM</h4>

The IG-LLM model is based on an instruction-tuned variant of LLaMA-1 7B in conjunction with a frozen CLIP vision encoder, used as the visual tokenizer. 

<h4 id="training">Training the IG-LLM / Vision-Language Alignment</h4>

The linear vision-encoder projection is initially trained using the feature-alignment pre-training method from LLaVA. Instruction sequences created from image-caption pairings taken from the Conceptual Captions dataset are used in this training. During training, the language model (LLM) receives an image embedding produced by the vision encoder, along with a randomly selected prompt that instructs the model to describe both the image and its caption. Throughout this pre-training stage, all parameters of the model remain unchanged, except for the learnable vision projector.


<h4 id="trainingdata">Training Data Generation</h4>

The procedurally created dataset CLEVR consists of basic three-dimensional objects arranged on a plane. The shape (such as spheres, cubes, etc.), size, color, and spatial pose of these objects, also known as primitives, are all randomly chosen. Shape, size, color, and material are discrete qualities; on the other hand, the pose, which indicates the object's location and orientation, is a continuous property. The images from the sampled scenes are rendered using the Blender modeling software from its Python scripting API.

To train the model, pairs of codes and their corresponding images are given to the model, and the model is tasked with figuring what code could be used to reproduce the image in the given pair. After that, the model is continued to train with a traditional next-token prediction task, with the goal of maximizing the conditional probability of the next token.

<img src="/images/pifalanfilan.png" alt="formula1" align="center" width="200"/>

Where $$s_i$$ represents the $$i$$ th token.
