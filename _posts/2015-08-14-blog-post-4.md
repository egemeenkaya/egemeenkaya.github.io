---
title: 'Re-Thinking Inverse Graphics With Large Language Models'
date: 04/2024
permalink: /posts/2012/08/ig-llm-egemenkaya/
tags:
  - Inverse Graphics
  - LLMs
  - Spatial Reasoning
---

Inverse Graphics (IG) is the process of disentangling an image into physical variables that contain information about its constituent properties such as its shape, color, material, position, rotation etc. This task is often equated with "analysis by synthesis" and is needed to recreate 3D scenes/environments from 2D images. It is a very common process, used in numerous fields, such as; Autonomous driving, computer vision, augmented reality and any system, where it is required to understand and process a 3D environment's properties through 2D images. 

<img src="/images/teapot.png" alt="teapot" align="center" width="600"/>

In the recent past, different approaches to solve this problem has been researched. In this blog post, we are going to take a look at a new approach that tries to solve more generalized inverse graphics problems: Inverse-Graphics Large Language Model (IG-LLM), from the paper "Re-Thinking Inverse Graphics With Large Language Models".

<h3 id="why llms">What is needed to "recreate" a scene?</h3>

In order to recreate a scene/environment seen in a 2D picture, there are some core steps a model has to take.
  First, the picture needs to be classified and "understood". What that means is that the model has to be able to define the objects in the picture and tell them apart by a sort of generalization algorithm. There are many ways to accomplish this task such as; pixel-level, object-level and scene-level classification. Usually, features are extracted from the image using convolutional neural networks (CNNs) or other techniques. For now, this is all we need to know about how classification works.

<img src="/images/classification.png" alt="classification" align="center" width="600"/>

  After understanding what the objectsin the image are, the relations between those objects must be inspected. Because the images are 2D planes with no depth, it is crucial to develop the understanding of depth before analyzing the spacing in the images. That is called 'holistic scene de-rendering' and how it operates is basically connecting objects with each others, using their spatial properties and other relations built before. This allows the models to understand the correct 3D setting of the objects from a 2D image.

<img src="/images/holisticscene.png" alt="holisticscene" align="center" width="600"/>

<h3 id="why llms">Neural Scene De-Rendering</h3>

The most basic way to solve IG problems is by using Neural Scene De-Rendering. It utilizes a neural network, often a convolutional neural network (CNN) or a more advanced architecture like a Generative Adversarial Network (GAN) to analyze the image. First, a CNN or GAN is utilized to recignize features in the image such as edges of objects, colors and textures. Then, in the feature embedding layer, it converts those high-level features into a fixed-dimensional vector or embedding. After that, through the usage of recurrent neural networks (RNNs) or transformers, it translates the visual features into symbolic representations. And lastly, through multi-layer perceptrons (MLPs) and other specialized networks, it reconstructs the 3D scene from the symbolic and numeric information.

<img src="/images/neuralscene.png" alt="neuralscene" align="center" width="600"/>

At this point it looks excellent. But if even the core approach of neural scene de-rendering is so capable by itself, why bother looking for other solutions to IG problems?
The answer to that question would be a single word: "generalizability". Even though the standard neural scene de-rendering approach by itself is already perfectly suited for many use-cases, it all falls apart when it is used with any kind of data it was not spesifically trained with. Because it is a simple framework/pipeline, it is not generalizable and does not have any zero-shot capabilities. 

The goal of the IG-LLM model is to leverage the proficiency of Large Language Models (LLMs) as LLMs exhibit compositional reasoning abilities that could be beneficial for spatial reasoning in 3D tasks. It is an inverse graphics framework centered around an LLM, that transforms a visual embedding into a structured, compositional 3D-scene representation autoregressively. This process works beyond mere pixel- or object-level interpretation of an image and tries to find physical relationships among essential objects of the scene, and with that, has the potential to be more generalizable and accurate than all of the previous approaches.

<h3 id="why llms">Large Language Models in Vision-Language</h3>

As known, one of the greatest abilities of LLMs is their exceptional ability to generalize. Unlike traditional approaches, LLMs are primarily trained via and unsupervised next-token-prediction, thereby unifying various natural language processing (NLP) tasks within a generic framework and not needing to scale the amount of task-spesific training data. There are many tasks in the domain of vision-language where LLMs are utilized such as: Generating textual descriptions from images or invice versa, visual question answering (VQA), visual instruction following etc.

In this research, through the use of an incorporated frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training, IG-LLM solves the IG problem through next-token prediction without needing to use image-space supervision, thus opening up new possibilities to exploit the visual knowledge of LLMs for solving IG problems. 

Inverse-Graphics Large Language Model
------

While LLMs create complete language-token sequences, past visual question answering (VQA) works have shown that large pretrained vision transformers can also be used as visual tokenizers. By fusing linguistic tokens for the LLM with visual embeddings, these studies integrate the understanding of images and words. They take an approach that is consistent with this method, building an LLM that can "see" the input image and provide a structured code representation of the input scene. IG-LLM is a model that takes advantage of this capability of LLMs and uses it to make logical reasonings that help through the process of understanding and recreating an image. 

<h3 id="architecture">Architecture of IG-LLM</h3>

The IG-LLM model is built upon the concept of neural scene de-rendering, with an instruction-tuned variant of LLaMA-1 7B in conjunction with a frozen CLIP vision encoder, used as the visual tokenizer. The layers of IG-LLM are as following:

* Region-proposal network for object detection
* Segmentation network for isolating the objects from the background
* Attribute network for classifying various discrete graphics attributes
* Localization network for predicting the object's spatial location
* Continuous numeric head
* An instruction tuned variant of LLaMA-1 7B

<img src="/images/ig-llm-blenderpipe.png" alt="IG-LLM Example" align="center" width="600"/>

<h3 id="architecture">How IG-LLM works</h3>

First, a frozen pre-trained visual encoder "CLIP" creates visual embeddings for the objects in the 2D input image (Neural Scene De-Rendering). The embeddings are numerical vectors with values that represent the attributes of the objects recognized in the image, such as shape, coordinates, size, color etc. Then, the instruction tuned variant of LLaMa-1 7B translates these visual embeddings into graphics code, using autoregressive prediction and contextual clues. That means, instead of translating every object's embedding without reference to each other, it starts from one object and translates them one after another while also using context and building logical reasoning around the objects. The LLM translates the textual properties such as size, color, material and shape. 

At the same time, the continuous numeric head predicts the exact numbers for the non-discrete properties such as location and rotation. It is implemented as a four-layer Multi-Layer Perceptron (MLP) that processes the final hidden-layer output of the LLM and converts it into scalar values. To enable the LLM to distinguish between the generation of numerical values and textual information, a dedicated token in the vocabulary is introduced: [NUM]. This token acts as a mask to signal when a numerical value should be produced. During training, a Mean Squared Error (MSE) loss is applied to each numerical value generated, in addition to the next-token prediction loss applied to the [NUM] token itself, as described in Equation below.

<img src="/images/pifalanfilan.png" alt="formula1" align="center" width="230"/>

And lastly, the result image is rendered using the Blender modeling software from its Python scripting API, through the graphics code generated by IG-LLM.

<h3 id="trainingdata">Training Data Generation 'CLEVR'</h3>

The procedurally created dataset CLEVR consists of basic three-dimensional objects arranged on a plane. The shape (such as spheres, cubes, etc.), size, color, and spatial pose of these objects, also known as primitives, are all randomly chosen. Shape, size, color, and material are discrete qualities (for the LLM); on the other hand, the pose, which indicates the object's location and orientation, is a continuous property (for the numeric head). Unlike the original CLEVR data, the training data for IG-LLM is created with fixed camera positions, but the randomness in the lighting is still maintained.

<img src="/images/cleversample.png" alt="clevrsample" align="center" width="600"/>

To train the model, pairs of codes and their corresponding images are given to the model, and the model is tasked with figuring what code could be used to reproduce the image in the given pair. After that, the model is continued to train with a traditional next-token prediction task, with the goal of maximizing the conditional probability of the next token.

<h3 id="training">Training IG-LLM and Vision-Language Alignment</h3>

The linear vision-encoder projection is initially trained using the feature-alignment pre-training method from LLaVA. Instruction sequences created from image-caption pairings taken from the Conceptual Captions dataset are used in this training. During training, the language model (LLM) receives an image embedding produced by the vision encoder, along with a randomly selected prompt that instructs the model to describe both the image and its caption. Throughout this pre-training stage, all parameters of the model remain unchanged, except for the learnable vision projector. 

To train IG-LLM model itself, the authors utilized several loss functions. The perceptual loss measures the high-level similarity between generated and real images, ensuring visually appealing results. Reconstruction loss ensures that the generated images closely match the original inputs, while the discriminator loss, applied in a GAN framework, enhances the realism of the output by differentiating between real and synthetic images. The models for the CLEVR and parameter-space generalization experiments are trained for 40k steps. The 6-DoF pose-estimation models are trained for 200k steps.

Evaluations
------


<h3 id="compgenerclevr">Compositional Generarlization on CLEVR</h3>

<h3 id="numericparameterspace">Numeric Parameter-Space Generalization</h3>

<h3 id="6dofpose">6-DoF Pose Estimation</h3>

Conclusion
------
